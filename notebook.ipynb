{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "| model |\n",
    "| --- |\n",
    "| gpt-3.5-turbo-0125 |\n",
    "| gpt-4-0125-preview |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo-0125\",\n",
    "    temperature = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='서울과 도쿄의 직선 거리는 약 1,200km 정도입니다. 제 이름은 루카스입니다.')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a geography expert. And you only reply in {language}\"),\n",
    "    AIMessage(content=\"Hi my name is {name}!\"),\n",
    "    HumanMessage(content=\"서울과 도쿄의 거리는 얼마인가요. 너의 이름은 뭐니?\"),\n",
    "]\n",
    "\n",
    "chat.predict_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'한국과 미국 사이의 거리는 대략 10,000km입니다. 제 이름은 AiGENDA입니다.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo-0125\",\n",
    "    temperature = 0.1\n",
    ")\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a geography expert. And you only reply in {language}.\"),\n",
    "        (\"ai\", \"Hi my name is {name}!\"),\n",
    "        (\"human\", \"What is the distance between {country_a} and {country_b}. Also, what is your name?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    language = \"Korean\",\n",
    "    name = \"AiGENDA\",\n",
    "    country_a = \"Korea\",\n",
    "    country_b = \"USA\"\n",
    ")\n",
    "\n",
    "chat.predict_messages(prompt).content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 OutputParser and LCEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. 손흥민',\n",
       " '2. 황의조',\n",
       " '3. 김민재',\n",
       " '4. 김영권',\n",
       " '5. 김민규',\n",
       " '6. 김진수',\n",
       " '7. 김영광',\n",
       " '8. 이재성',\n",
       " '9. 황인범',\n",
       " '10. 김승대']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    def parse(self, text):\n",
    "        items = text.strip().split(\",\")\n",
    "        \n",
    "        return list(map(str.strip, items))\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo-0125\",\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a list generating machine. Everything you are asked will be answered with a comma separated list of max {max_items}. Do NOT reply with anything else. And you only reply in {language}.\"),\n",
    "        (\"ai\", \"Hi my name is {name}!\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    name = \"aigenda\",\n",
    "    language = \"Korean\",\n",
    "    max_items = 10,\n",
    "    question = \"2024 한국 축구 국가대표 맴버\",\n",
    ")\n",
    "\n",
    "result = chat.predict_messages(prompt)\n",
    "\n",
    "p = CommaOutputParser()\n",
    "\n",
    "p.parse(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['손흥민', '황의조', '김민재', '김영권', '김민재']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a list generating machine. Everything you are asked will be answered with a comma separated list of max {max_items}. Do NOT reply with anything else. And you only reply in {language}.\"),\n",
    "        (\"ai\", \"Hi my name is {name}!\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo-0125\",\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    def parse(self, text):\n",
    "        items = text.strip().split(\",\")\n",
    "        \n",
    "        return list(map(str.strip, items))\n",
    "\n",
    "chain = template | chat | CommaOutputParser()    \n",
    "\n",
    "chain.invoke({\n",
    "    \"name\":\"aigenda\",\n",
    "    \"language\":\"Korean\",\n",
    "    \"max_items\":5,\n",
    "    \"question\":\"2024 한국 축구 국가대표 맴버\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Chaining Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kimchi, bulgogi, bibimbap, tteokbokki김치는 대체할 수 있는 재료가 없습니다. \n",
      "\n",
      "불고기를 채식주의자용으로 만들려면 대체 고기로 대신할 수 있습니다. 대체 고기로는 대부분 식물성 단백질이나 버섯을 사용합니다. 대체 고기를 사용할 때는 불고기 소스와 함께 볶아내면 됩니다.\n",
      "\n",
      "비빔밥은 채소나 과일을 활용하여 대체할 수 있습니다. 대체 재료로는 아보카도, 콩, 토마토, 당근, 오이 등을 사용할 수 있습니다. 이를 채소와 함께 밥 위에 올려 비빔밥 소스와 함께 섞어먹으면 됩니다.\n",
      "\n",
      "떡볶이는 떡 대신에 떡 대신에 고구마, 양배추, 어묵, 버섯 등을 사용하여 대체할 수 있습니다. 떡 대신에 이러한 채소나 버섯을 썰어서 떡볶이 소스와 함께 볶아내면 됩니다."
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='김치는 대체할 수 있는 재료가 없습니다. \\n\\n불고기를 채식주의자용으로 만들려면 대체 고기로 대신할 수 있습니다. 대체 고기로는 대부분 식물성 단백질이나 버섯을 사용합니다. 대체 고기를 사용할 때는 불고기 소스와 함께 볶아내면 됩니다.\\n\\n비빔밥은 채소나 과일을 활용하여 대체할 수 있습니다. 대체 재료로는 아보카도, 콩, 토마토, 당근, 오이 등을 사용할 수 있습니다. 이를 채소와 함께 밥 위에 올려 비빔밥 소스와 함께 섞어먹으면 됩니다.\\n\\n떡볶이는 떡 대신에 떡 대신에 고구마, 양배추, 어묵, 버섯 등을 사용하여 대체할 수 있습니다. 떡 대신에 이러한 채소나 버섯을 썰어서 떡볶이 소스와 함께 볶아내면 됩니다.')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo-0125\",\n",
    "    temperature = 0.1,\n",
    "    streaming=True,\n",
    "    callbacks = [StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "\n",
    "chef_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a list generating machine. \\\n",
    "         Everything you are asked will be answered with a comma separated list of max 5 in lowercase. \\\n",
    "         Do NOT reply with anything else.\"),\n",
    "        # (\"human\", \"{question}\"),\n",
    "        (\"human\", \"I want to cook {cuisine} food.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "veg_chef_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a vegetarian chef specialized on making traditional recipies vegetarian. \\\n",
    "         You find alternative ingredients and explain their preparation. \\\n",
    "         You don't radically modify the recipe. \\\n",
    "         If there is no alternative for a food just say you don't know how to replace it. \\\n",
    "         And you only reply in Korean\"),\n",
    "        (\"human\", \"{recipe}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chef_chain = chef_prompt | chat\n",
    "\n",
    "veg_chain = veg_chef_prompt | chat\n",
    "\n",
    "final_chain = {\"recipe\": chef_chain} | veg_chain\n",
    "\n",
    "final_chain.invoke({\"cuisine\":\"korea\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model IO\n",
    "\n",
    "https://python.langchain.com/docs/modules/model_io/\n",
    "\n",
    "https://python.langchain.com/docs/modules/data_connection/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI : \n",
      "        I know this:\n",
      "        Capital: 레이캬비크\n",
      "        Language: 아이슬란드어\n",
      "        Food: 스모크드 피쉬, 람\n",
      "        Currency: 아이슬란드 크로나"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='AI : \\n        I know this:\\n        Capital: 레이캬비크\\n        Language: 아이슬란드어\\n        Food: 스모크드 피쉬, 람\\n        Currency: 아이슬란드 크로나')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo-0125\",\n",
    "    temperature = 0.1,\n",
    "    streaming=True,\n",
    "    callbacks = [StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: 파리\n",
    "        Language: 불어\n",
    "        Food: 와인, 치즈\n",
    "        Currency: 유로화\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: 로마\n",
    "        Language: 이태리어\n",
    "        Food: 피자, 파스타\n",
    "        Currency: 유로화\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: 아테네\n",
    "        Language: 그리스어\n",
    "        Food: 수블라키 and 페타 치즈\n",
    "        Currency: 유로화\n",
    "        \"\"\"\n",
    "    },\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human : {question}\\nAI : {answer}\")\n",
    "    \n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt = example_prompt,\n",
    "    examples = examples,\n",
    "    suffix = \"Human : What do you know about {country}?\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "chain = prompt | chat\n",
    "\n",
    "chain.invoke({\n",
    "    \"country\" : \"아이슬란드\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 FewShotChatMessagePromptTemplate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 : input_variables=['country'] messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a geography expert, you give short answers.')), FewShotChatMessagePromptTemplate(examples=[{'country': 'France?', 'answer': '\\n        Here is what I know:\\n        Capital: 파리\\n        Language: 불어\\n        Food: 와인, 치즈\\n        Currency: 유로화\\n        '}, {'country': 'Italy?', 'answer': '\\n        I know this:\\n        Capital: 로마\\n        Language: 이태리어\\n        Food: 피자, 파스타\\n        Currency: 유로화\\n        '}, {'country': 'Greece?', 'answer': '\\n        I know this:\\n        Capital: 아테네\\n        Language: 그리스어\\n        Food: 수블라키 and 페타 치즈\\n        Currency: 유로화\\n        '}], example_prompt=ChatPromptTemplate(input_variables=['answer', 'country'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['country'], template='What do you know about {country}?')), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['answer'], template='{answer}'))])), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['country'], template='What do you know about {country}?'))]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        I know this:\n",
      "        Capital: 방콕\n",
      "        Language: 태국어\n",
      "        Food: 태국 카레, 생선 찌개\n",
      "        Currency: 태국 바트\n",
      "        "
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='\\n        I know this:\\n        Capital: 방콕\\n        Language: 태국어\\n        Food: 태국 카레, 생선 찌개\\n        Currency: 태국 바트\\n        ')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import ChatMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo-0125\",\n",
    "    temperature = 0.1,\n",
    "    streaming=True,\n",
    "    callbacks = [StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"country\": \"France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: 파리\n",
    "        Language: 불어\n",
    "        Food: 와인, 치즈\n",
    "        Currency: 유로화\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: 로마\n",
    "        Language: 이태리어\n",
    "        Food: 피자, 파스타\n",
    "        Currency: 유로화\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: 아테네\n",
    "        Language: 그리스어\n",
    "        Food: 수블라키 and 페타 치즈\n",
    "        Currency: 유로화\n",
    "        \"\"\"\n",
    "    },\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"What do you know about {country}?\"),\n",
    "        (\"ai\", \"{answer}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 예시 문장과 실제 예시를 보여줌   \n",
    "example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "# system : 전달 인자, example_prompt : 예시 문장과 실제 예시 프롬프트, human : 물어볼 메시지\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a geography expert, you give short answers.\"),\n",
    "        example_prompt,\n",
    "        (\"human\", \"What do you know about {country}?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"3 : {final_prompt}\")\n",
    "\n",
    "chain = final_prompt | chat\n",
    "\n",
    "chain.invoke({\"country\": \"Thailand\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 LengthBasedExampleSelector\n",
    "\n",
    "example의 수를 제한해서 비용을 줄일 수 있는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human : What do you know about Italy?\\nAI : \\n        I know this:\\n        Capital: 로마\\n        Language: 이태리어\\n        Food: 피자, 파스타\\n        Currency: 유로화\\n        \\n\\nHuman : What do you know about 브라질?'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import ChatMessagePromptTemplate, ChatPromptTemplate\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo-0125\",\n",
    "    temperature = 0.1,\n",
    "    streaming=True,\n",
    "    callbacks = [StreamingStdOutCallbackHandler()],\n",
    ")\n",
    "\n",
    "class RandomExampleSelector(BaseExampleSelector):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "\n",
    "    def add_example(self, example):\n",
    "        self.examples.append(example)\n",
    "\n",
    "    def select_examples(self, input_variables):\n",
    "        from random import choice\n",
    "\n",
    "        return [choice(self.examples)]\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: 파리\n",
    "        Language: 불어\n",
    "        Food: 와인, 치즈\n",
    "        Currency: 유로화\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: 로마\n",
    "        Language: 이태리어\n",
    "        Food: 피자, 파스타\n",
    "        Currency: 유로화\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: 아테네\n",
    "        Language: 그리스어\n",
    "        Food: 수블라키 and 페타 치즈\n",
    "        Currency: 유로화\n",
    "        \"\"\"\n",
    "    },\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human : {question}\\nAI : {answer}\")\n",
    "\n",
    "example_selector = RandomExampleSelector(\n",
    "    examples = examples,\n",
    ")\n",
    "    \n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt = example_prompt,\n",
    "    example_selector = example_selector,\n",
    "    suffix = \"Human : What do you know about {country}?\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "prompt.format(country=\"브라질\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Serialization and Composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the capital of xxx'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import load_prompt\n",
    "\n",
    "prompt = load_prompt(\"./prompt.json\")\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo-0125\",\n",
    "    temperature = 0.1,\n",
    "    streaming=True,\n",
    "    callbacks = [StreamingStdOutCallbackHandler()],\n",
    ")\n",
    "\n",
    "prompt.format(country=\"xxx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrrrg matey! Me favorite grub be a hearty plate o' salted beef and hardtack! Aye, it be a meal fit for a pirate like meself! Arrrrg!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content=\"Arrrrg matey! Me favorite grub be a hearty plate o' salted beef and hardtack! Aye, it be a meal fit for a pirate like meself! Arrrrg!\")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo-0125\",\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "intro = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a role playing assistant.\n",
    "    And you are impersonating a {character}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "example = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    This is an example of how you talk:\n",
    "\n",
    "    Human: {example_question}\n",
    "    You: {example_answer}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "start = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Start now!\n",
    "\n",
    "    Human: {question}\n",
    "    You:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "final = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    {intro}\n",
    "                                     \n",
    "    {example}\n",
    "                              \n",
    "    {start}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    (\"intro\", intro),\n",
    "    (\"example\", example),\n",
    "    (\"start\", start),\n",
    "]\n",
    "\n",
    "\n",
    "full_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=final,\n",
    "    pipeline_prompts=prompts,\n",
    ")\n",
    "\n",
    "\n",
    "chain = full_prompt | chat\n",
    "\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"character\": \"Pirate\",\n",
    "        \"example_question\": \"What is your location?\",\n",
    "        \"example_answer\": \"Arrrrg! That is a secret!! Arg arg!!\",\n",
    "        \"question\": \"What is your fav food?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: How do you make italian pizza\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] [2ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"To make Italian pizza, you will need the following ingredients:\\n\\n- 2 1/4 cups of all-purpose flour\\n- 1 teaspoon of salt\\n- 1 teaspoon of sugar\\n- 1 packet of active dry yeast\\n- 1 cup of warm water\\n- 2 tablespoons of olive oil\\n- Tomato sauce\\n- Mozzarella cheese\\n- Toppings of your choice (such as pepperoni, mushrooms, bell peppers, etc.)\\n\\nHere is a step-by-step guide to making Italian pizza:\\n\\n1. In a large mixing bowl, combine the flour, salt, and sugar. In a separate small bowl, dissolve the yeast in the warm water and let it sit for about 5 minutes until it becomes frothy.\\n\\n2. Pour the yeast mixture and olive oil into the flour mixture and stir until a dough forms. Knead the dough on a floured surface for about 5-7 minutes until it becomes smooth and elastic.\\n\\n3. Place the dough in a greased bowl, cover it with a clean kitchen towel, and let it rise in a warm place for about 1-2 hours, or until it has doubled in size.\\n\\n4. Preheat your oven to 475°F (245°C) and place a pizza stone or baking sheet in the oven to heat up.\\n\\n5. Punch down the dough and divide it into two equal portions. Roll out each portion into a round pizza crust on a floured surface.\\n\\n6. Place the pizza crust on a pizza peel or parchment paper and top it with tomato sauce, mozzarella cheese, and your choice of toppings.\\n\\n7. Slide the pizza onto the preheated pizza stone or baking sheet in the oven and bake for about 12-15 minutes, or until the crust is golden brown and the cheese is melted and bubbly.\\n\\n8. Remove the pizza from the oven, slice it, and serve hot. Enjoy your delicious homemade Italian pizza!\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"To make Italian pizza, you will need the following ingredients:\\n\\n- 2 1/4 cups of all-purpose flour\\n- 1 teaspoon of salt\\n- 1 teaspoon of sugar\\n- 1 packet of active dry yeast\\n- 1 cup of warm water\\n- 2 tablespoons of olive oil\\n- Tomato sauce\\n- Mozzarella cheese\\n- Toppings of your choice (such as pepperoni, mushrooms, bell peppers, etc.)\\n\\nHere is a step-by-step guide to making Italian pizza:\\n\\n1. In a large mixing bowl, combine the flour, salt, and sugar. In a separate small bowl, dissolve the yeast in the warm water and let it sit for about 5 minutes until it becomes frothy.\\n\\n2. Pour the yeast mixture and olive oil into the flour mixture and stir until a dough forms. Knead the dough on a floured surface for about 5-7 minutes until it becomes smooth and elastic.\\n\\n3. Place the dough in a greased bowl, cover it with a clean kitchen towel, and let it rise in a warm place for about 1-2 hours, or until it has doubled in size.\\n\\n4. Preheat your oven to 475°F (245°C) and place a pizza stone or baking sheet in the oven to heat up.\\n\\n5. Punch down the dough and divide it into two equal portions. Roll out each portion into a round pizza crust on a floured surface.\\n\\n6. Place the pizza crust on a pizza peel or parchment paper and top it with tomato sauce, mozzarella cheese, and your choice of toppings.\\n\\n7. Slide the pizza onto the preheated pizza stone or baking sheet in the oven and bake for about 12-15 minutes, or until the crust is golden brown and the cheese is melted and bubbly.\\n\\n8. Remove the pizza from the oven, slice it, and serve hot. Enjoy your delicious homemade Italian pizza!\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'To make Italian pizza, you will need the following ingredients:\\n\\n- 2 1/4 cups of all-purpose flour\\n- 1 teaspoon of salt\\n- 1 teaspoon of sugar\\n- 1 packet of active dry yeast\\n- 1 cup of warm water\\n- 2 tablespoons of olive oil\\n- Tomato sauce\\n- Mozzarella cheese\\n- Toppings of your choice (such as pepperoni, mushrooms, bell peppers, etc.)\\n\\nHere is a step-by-step guide to making Italian pizza:\\n\\n1. In a large mixing bowl, combine the flour, salt, and sugar. In a separate small bowl, dissolve the yeast in the warm water and let it sit for about 5 minutes until it becomes frothy.\\n\\n2. Pour the yeast mixture and olive oil into the flour mixture and stir until a dough forms. Knead the dough on a floured surface for about 5-7 minutes until it becomes smooth and elastic.\\n\\n3. Place the dough in a greased bowl, cover it with a clean kitchen towel, and let it rise in a warm place for about 1-2 hours, or until it has doubled in size.\\n\\n4. Preheat your oven to 475°F (245°C) and place a pizza stone or baking sheet in the oven to heat up.\\n\\n5. Punch down the dough and divide it into two equal portions. Roll out each portion into a round pizza crust on a floured surface.\\n\\n6. Place the pizza crust on a pizza peel or parchment paper and top it with tomato sauce, mozzarella cheese, and your choice of toppings.\\n\\n7. Slide the pizza onto the preheated pizza stone or baking sheet in the oven and bake for about 12-15 minutes, or until the crust is golden brown and the cheese is melted and bubbly.\\n\\n8. Remove the pizza from the oven, slice it, and serve hot. Enjoy your delicious homemade Italian pizza!'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.globals import set_llm_cache, set_debug\n",
    "from langchain.cache import InMemoryCache, SQLiteCache\n",
    "\n",
    "set_llm_cache(SQLiteCache(\"cache.db\"))\n",
    "set_debug(True)\n",
    "\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo-0125\",\n",
    "    temperature=0.1,\n",
    "    # streaming=True,\n",
    "    # callbacks=[\n",
    "    #     StreamingStdOutCallbackHandler(),\n",
    "    # ],\n",
    ")\n",
    "\n",
    "chat.predict(\"How do you make italian pizza\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To make toast, you will need the following ingredients and equipment:\n",
      "\n",
      "Ingredients:\n",
      "- Bread slices\n",
      "- Butter or margarine (optional)\n",
      "\n",
      "Equipment:\n",
      "- Toaster or toaster oven\n",
      "\n",
      "Instructions:\n",
      "1. Plug in your toaster or toaster oven and set it to the desired level of toasting (light, medium, or dark).\n",
      "2. Place the bread slices into the toaster slots or on the toaster oven tray.\n",
      "3. Press down the lever on the toaster or start the toaster oven.\n",
      "4. Wait \n",
      "\n",
      "Ingredients:\n",
      "- 2 slices of bread\n",
      "- 2 eggs\n",
      "- 1 tablespoon of butter\n",
      "- Salt and pepper to taste\n",
      "- Optional toppings: cheese, bacon, avocado, tomato, spinach\n",
      "\n",
      "Instructions:\n",
      "1. Heat a non-stick skillet over medium heat and melt the butter.\n",
      "2. Crack the eggs into the skillet and season with salt and pepper.\n",
      "3. Cook the eggs to your desired doneness, either scrambled, fried, or poached.\n",
      "4. While the eggs \n",
      "\n",
      "Tokens Used: 227\n",
      "\tPrompt Tokens: 27\n",
      "\tCompletion Tokens: 200\n",
      "Successful Requests: 2\n",
      "Total Cost (USD): $0.0\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "# from langchain.globals import set_llm_cache, set_debug\n",
    "# from langchain.cache import SQLiteCache\n",
    "\n",
    "# set_llm_cache(SQLiteCache(\"cache.db\"))\n",
    "# set_debug(True)\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo-0125\",\n",
    "    temperature=0.1,\n",
    "    max_tokens= 1000,\n",
    ")\n",
    "\n",
    "\n",
    "with get_openai_callback() as usage:\n",
    "    a = chat.predict(\"What is the recipe for toast\")\n",
    "    b = chat.predict(\"What is the recipe for egg sandwich\")\n",
    "    print(a, \"\\n\")\n",
    "    print(b, \"\\n\")\n",
    "    print(usage)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
