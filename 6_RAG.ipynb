{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 RAG\n",
    "\n",
    "Retrieval Augumented Generation(검색 증강 생성)\n",
    "\n",
    "질문을 개인의 문서와 함께 prompt에서 합치는 방법\n",
    "\n",
    "<img src=\"https://python.langchain.com/assets/images/data_connection-95ff2033a8faa5f3ba41376c0f6dd32a.jpg\">\n",
    "\n",
    "Load : 어떤 소스에서 데이터를 받아옴\n",
    "\n",
    "Transform : 받아온 데이터를 분할\n",
    "\n",
    "Embed : 데이터를 컴퓨터가 알아듣기 쉽게 벡터화\n",
    "\n",
    "Store : 벡터 데이터를 저장 (Vector Store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Data Loaders and Splitters\n",
    "\n",
    "RAG의 첫번째 단계 : 데이터 검색\n",
    "\n",
    "<https://python.langchain.com/docs/modules/data_connection/document_loaders/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "\n",
    "# splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=200,\n",
    "#     chunk_overlap=50\n",
    "# )\n",
    "\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.pdf\")\n",
    "\n",
    "loader.load_and_split(text_splitter=splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Tiktoken\n",
    "<https://platform.openai.com/tokenizer>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Vectors\n",
    "\n",
    "Embedding : 사람이 읽는 텍스트를 컴퓨터가 쉽게 이해할 수 있도록 변환하는 과정\n",
    "\n",
    "각각의 문서를 Vectorization화함\n",
    "\n",
    "연습에서는 3차원 백터로 단어를 표현\n",
    "\n",
    "openAI에서는 1536차원의 벡터로 단어를 \n",
    "\n",
    "<https://turbomaze.github.io/word2vecjson/>\n",
    "\n",
    "<https://www.youtube.com/watch?v=2eWuYf-aZE4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masculinity | Femininity | Royalty\n",
    "\n",
    "king    | 0.9   | 0.1   | 1.0\n",
    "queen   | 0.1   | 0.9   | 1.0\n",
    "man     | 0.9   | 0.1   | 0.0\n",
    "woman   | 0.1   | 0.9   | 0.0\n",
    "\n",
    "royal = king - man | 0.0    | 0.0   | 1.0\n",
    "queen = royal + woman   | 0.1   | 0.9   | 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embedder = OpenAIEmbeddings()\n",
    "\n",
    "vector = embedder.embed_documents([\n",
    "    \"hi\",\n",
    "    \"how\",\n",
    "    \"are\",\n",
    "    \"you\",\n",
    "    \"longer sentences because\"\n",
    "])\n",
    "\n",
    "len(vector[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chroma를 이용하여 임배딩\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import LocalFileStore\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "# loader = UnstructuredFileLoader(\"./files/lucky_day.txt\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embedding = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings, cache_dir,\n",
    ")\n",
    "\n",
    "vectorstore = Chroma.from_documents(docs, cached_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vectorstore.similarity_search(\"주인공은 왜 기분이 좋은가?\")\n",
    "\n",
    "results\n",
    "# len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Langsmith\n",
    "\n",
    "<https://www.langchain.com/langsmith>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 RetrievalQA\n",
    "\n",
    "생성한 문서를 이용하여 LLM에게 질문함\n",
    "\n",
    "off-the-shelf chain : 미리 만들어 놓은 chain을 사용함\n",
    "\n",
    "#### Stuff document chain Method\n",
    "<https://js.langchain.com/docs/modules/chains/document/stuff>\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GykpE5AdrPkT2jPM3XH-QQ.png\">\n",
    "\n",
    "docs와 question을 prompt에 묶어서 한번에 질문\n",
    "\n",
    "---\n",
    "\n",
    "#### Refine document chain\n",
    "<https://js.langchain.com/docs/modules/chains/document/refine>\n",
    "\n",
    "<img src=\"https://js.langchain.com/assets/images/refine-a70f30dd7ada6fe5e3fcc40dd70de037.jpg\">\n",
    "\n",
    "여러개의 document를 순차적으로 질문하고 이를 반복하면서 정제된 답변을 도출\n",
    "\n",
    "Stuff document chain 보다 더 비쌈\n",
    "\n",
    "---\n",
    "\n",
    "#### Map reduce document chain\n",
    "<https://js.langchain.com/docs/modules/chains/document/map_reduce>\n",
    "\n",
    "<img src=\"https://js.langchain.com/assets/images/map_reduce-c65525a871b62f5cacef431625c4d133.jpg\">\n",
    "\n",
    "document를 입력받아서, 개별적으로 요약작업 수행\n",
    "\n",
    "각각의 요약본을 LLM에 전달\n",
    "\n",
    "매우 많은 연산작업이 필요\n",
    "\n",
    "---\n",
    "\n",
    "### Map re-rank document chain (현재 안됨)\n",
    "<https://python.langchain.com.cn/docs/modules/chains/document/map_rerank>\n",
    "\n",
    "<img src=\"https://python.langchain.com.cn/assets/images/map_rerank-0302b59b690c680ad6099b7bfe6d9fe5.jpg\">\n",
    "\n",
    "각 document를 입력받아서, 개별적으로 요약작업을 수행하고 점수(score)를 매김\n",
    "\n",
    "가장 높은 점수인 답변을 점수와 함께 도출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorstore : Chroma 방식\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "# loader = UnstructuredFileLoader(\"./files/lucky_day.txt\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embedding = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings, cache_dir,\n",
    ")\n",
    "\n",
    "vectorstore = Chroma.from_documents(docs, cached_embedding)\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm = llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.run(\"Who is a main character. Tell me by Korean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorstore : FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "# loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "loader = UnstructuredFileLoader(\"./files/lucky_day.txt\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embedding = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings, cache_dir,\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cached_embedding)\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm = llm,\n",
    "    chain_type=\"refine\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.run(\"Who is a main character. Tell me by Korean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm = llm,\n",
    "    chain_type=\"map_reduce\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    ")\n",
    "\n",
    "chain.run(\"여기에 등장하는 인물 모두 말해줘.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm = llm,\n",
    "    chain_type=\"map_reduce\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    ")\n",
    "\n",
    "chain.run(\"Describe Victory Mansions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.8 Stuff LCEL Chain\n",
    "\n",
    "LangChain Expression Language를 사용해서 stuff chain을 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "# loader = UnstructuredFileLoader(\"./files/lucky_day.txt\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embedding = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir,)\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cached_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Victory Mansions is a building with glass doors that Winston Smith enters. The hallway smells of boiled cabbage and old rag mats. It has seven flights of stairs, and the lift is seldom working. There is a large colored poster of a man's face on the wall, and the building is not well maintained, with the electric current cut off during daylight hours as part of an economy drive.\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo-0125\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "retriver = vectorstore.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant. \\\n",
    "        Answer questions using only the following context. \\\n",
    "        If you don't know the answer just say you don't know, \\\n",
    "        don't make it up:\\n\\n{context}\",\n",
    "    ),\n",
    "    (\"human\", \"{question}\",),\n",
    "])\n",
    "\n",
    "chain = {\"context\":retriver, \"question\":RunnablePassthrough()} | prompt | llm\n",
    "\n",
    "chain.invoke(\"Describe Victory Mansions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The main character is Winston Smith.')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo-0125\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "retriver = vectorstore.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant. \\\n",
    "        Answer questions using only the following context. \\\n",
    "        If you don't know the answer just say you don't know, \\\n",
    "        don't make it up:\\n\\n{context}\",\n",
    "    ),\n",
    "    (\"human\", \"{question}\",),\n",
    "])\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\":retriver,\n",
    "        \"question\":RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "chain.invoke(\"who is main character\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.9 Map Reduce LCEL Chain\n",
    "\n",
    "LangChain Expression Language를 사용해서 Map Reduce chain을 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "# loader = UnstructuredFileLoader(\"./files/lucky_day.txt\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embedding = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir,)\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cached_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Winston goes to work at the Ministry of Truth, which is described as an enormous pyramidal structure of glittering white concrete, soaring up 300 meters into the air.')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo-0125\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "retriver = vectorstore.as_retriever()\n",
    "\n",
    "# 1 : list of docs\n",
    "# 2 : for doc in list of docs | prompt | llm\n",
    "# 3 : for respone in list of llms respons | put them all together\n",
    "# 4 : final doc | prompt | llm \n",
    "\n",
    "map_doc_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Use the following portion of a long document to see if any of the text is relevant to answer the question.\n",
    "            Return any relevant text verbatim.\n",
    "            If there is no relevant text, return : ''\n",
    "            -------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 2. for doc in list of docs | prompt | llm\n",
    "map_doc_chain = map_doc_prompt | llm\n",
    "\n",
    "# 3. for respone in list of llms respons | put them all together\n",
    "def map_docs(inputs):\n",
    "    documents = inputs['documents']\n",
    "    question = inputs['question']\n",
    "    # results = []\n",
    "    # for document in documents:\n",
    "    #     result = map_doc_chain.invoke({\n",
    "    #         \"context\": document.page_content,\n",
    "    #         \"question\": question\n",
    "    #     }).content\n",
    "    #     results.append(result)\n",
    "    # return \"\\n\\n\".join(results)\n",
    "    return \"\\n\\n\".join(\n",
    "        map_doc_chain.invoke(\n",
    "            {\"context\": doc.page_content, \"question\": question}\n",
    "        ).content\n",
    "        for doc in documents\n",
    "    )\n",
    "\n",
    "# 1. list of docs\n",
    "map_chain = {\"documents\":retriver, \"question\": RunnablePassthrough()} | RunnableLambda(map_docs)\n",
    "    \n",
    "# 4. final doc | prompt | llm \n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Given the following extracted parts of a long document and a question, create a final answer. \n",
    "            If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "            ------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": map_chain,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | final_prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "chain.invoke(\"Describe Vitory Mansions\")\n",
    "# chain.invoke(\"Where does Winston go to work?\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
